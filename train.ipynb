{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "import spacy\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaredxfeng/sides/nergnt/venvv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('grc_odycy_joint_trf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('acts.csv')\n",
    "df['Verse'] = df['Verse'].astype(int)\n",
    "df['Proper Noun'] = df['Proper Noun'].str.replace('TRUE,', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            Τὸν\n",
       "1            μὲν\n",
       "2         πρῶτον\n",
       "3          λόγον\n",
       "4     ἐποιησάμην\n",
       "5           περὶ\n",
       "6         πάντων\n",
       "7              ὦ\n",
       "8        Θεόφιλε\n",
       "9             ὧν\n",
       "10        ἤρξατο\n",
       "11             ὁ\n",
       "12        Ἰησοῦς\n",
       "13        ποιεῖν\n",
       "14            τε\n",
       "15           καὶ\n",
       "16     διδάσκειν\n",
       "17          ἄχρι\n",
       "18            ἧς\n",
       "19        ἡμέρας\n",
       "Name: Word, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Word'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Proper Noun\n",
       "PERSON    374\n",
       "GPE       177\n",
       "DIVINE    157\n",
       "GROUP     141\n",
       "TITLE     117\n",
       "ORG        22\n",
       "LOC        21\n",
       "DEITY      11\n",
       "EVENT       8\n",
       "BOOK        7\n",
       "LANG        2\n",
       "TIME        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Proper Noun'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10038"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Split train vs. test\n",
    "\n",
    "Take the first 7 chapters as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['Verse'].astype(int) < 50801]\n",
    "df_test = df[df['Verse'].astype(int) >= 50801]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate NER inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ner_inputs(df):\n",
    "    inputs, verse_ids = [], []\n",
    "    for _, df_verse in df.groupby('Verse'):\n",
    "        verse_id = df_verse['Verse'].iloc[0]\n",
    "        verse_ids.append(verse_id)\n",
    "        verse = ' '.join(df_verse['Word'])\n",
    "        df_verse['Word Size'] = df_verse['Word'].str.len()\n",
    "        df_verse['Character Count Cumsum'] = (df_verse['Word Size'] + 1).cumsum()\n",
    "        # df_verse.at[df_verse.index[-1], 'Character Count Cumsum']  1\n",
    "        df_verse['Word Start Index'] = df_verse['Character Count Cumsum'].shift(1).fillna(0).astype(int)\n",
    "        df_verse['Word End Index'] = df_verse['Character Count Cumsum'] - 1\n",
    "        entities = []\n",
    "        df_entities = df_verse[~df_verse['Proper Noun'].isna()]\n",
    "        for row_idx, row in df_entities.iterrows():\n",
    "            entities.append((row['Word Start Index'], row['Word End Index'], row['Proper Noun']))\n",
    "        inputs.append((verse, {'entities': entities}))\n",
    "    return inputs, verse_ids\n",
    "\n",
    "ner_train_inputs, train_verse_ids = generate_ner_inputs(df_train)\n",
    "ner_test_inputs, test_verse_ids = generate_ner_inputs(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Τὸν μὲν πρῶτον λόγον ἐποιησάμην περὶ πάντων ὦ Θεόφιλε ὧν ἤρξατο ὁ Ἰησοῦς ποιεῖν τε καὶ διδάσκειν',\n",
       "  {'entities': [(46, 53, 'PERSON'), (66, 72, 'PERSON')]}),\n",
       " ('ἄχρι ἧς ἡμέρας ἐντειλάμενος τοῖς ἀποστόλοις διὰ πνεύματος ἁγίου οὓς ἐξελέξατο ἀνελήμφθη',\n",
       "  {'entities': [(33, 43, 'GROUP'), (48, 63, 'DIVINE')]}),\n",
       " ('οἷς καὶ παρέστησεν ἑαυτὸν ζῶντα μετὰ τὸ παθεῖν αὐτὸν ἐν πολλοῖς τεκμηρίοις δι’ ἡμερῶν τεσσεράκοντα ὀπτανόμενος αὐτοῖς καὶ λέγων τὰ περὶ τῆς βασιλείας τοῦ θεοῦ',\n",
       "  {'entities': [(154, 158, 'DIVINE')]}),\n",
       " ('καὶ συναλιζόμενος παρήγγειλεν αὐτοῖς ἀπὸ Ἱεροσολύμων μὴ χωρίζεσθαι ἀλλὰ περιμένειν τὴν ἐπαγγελίαν τοῦ πατρὸς ἣν ἠκούσατέ μου',\n",
       "  {'entities': [(41, 52, 'GPE'), (102, 108, 'DIVINE')]}),\n",
       " ('ὅτι Ἰωάννης μὲν ἐβάπτισεν ὕδατι ὑμεῖς δὲ ἐν πνεύματι βαπτισθήσεσθε ἁγίῳ οὐ μετὰ πολλὰς ταύτας ἡμέρας',\n",
       "  {'entities': [(4, 11, 'PERSON'), (44, 52, 'DIVINE')]})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_train_inputs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer',\n",
       " 'tagger',\n",
       " 'morphologizer',\n",
       " 'parser',\n",
       " 'trainable_lemmatizer',\n",
       " 'frequency_lemmatizer']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an NER pipeline in odyCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_pipe = nlp.add_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ner' in nlp.pipe_names:\n",
    "    nlp.remove_pipe(\"ner\")\n",
    "ner_pipe = nlp.add_pipe(\"ner\")\n",
    "# ner_pipe = nlp.get_pipe('ner')\n",
    "for label in df['Proper Noun'].unique():\n",
    "    if str(label) != 'nan':\n",
    "        ner_pipe.add_label(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BOOK',\n",
       " 'DEITY',\n",
       " 'DIVINE',\n",
       " 'EVENT',\n",
       " 'GPE',\n",
       " 'GROUP',\n",
       " 'LANG',\n",
       " 'LOC',\n",
       " 'ORG',\n",
       " 'PERSON',\n",
       " 'TIME',\n",
       " 'TITLE')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_pipe.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the NER pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated pipeline: ['transformer', 'tagger', 'morphologizer', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "if \"frequency_lemmatizer\" in nlp.pipe_names:\n",
    "    nlp.remove_pipe(\"frequency_lemmatizer\")\n",
    "if \"trainable_lemmatizer\" in nlp.pipe_names:\n",
    "    nlp.remove_pipe(\"trainable_lemmatizer\")\n",
    "print(\"Updated pipeline:\", nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['parser', 'transformer', 'tagger', 'morphologizer']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disable other pipeline components during training (if any)\n",
    "enabled_pipes = ['ner']\n",
    "other_pipes = list(set(nlp.pipe_names) - set(enabled_pipes))\n",
    "other_pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ner_train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Losses: {'ner': 745.3154391540772}\n",
      "Iteration 1, Losses: {'ner': 584.0769424879805}\n",
      "Iteration 2, Losses: {'ner': 457.5183563799422}\n",
      "Iteration 3, Losses: {'ner': 674.4264358493301}\n",
      "Iteration 4, Losses: {'ner': 448.86567710825176}\n",
      "Iteration 5, Losses: {'ner': 335.05419278878526}\n",
      "Iteration 6, Losses: {'ner': 305.8969668723571}\n",
      "Iteration 7, Losses: {'ner': 293.32673063683245}\n",
      "Iteration 8, Losses: {'ner': 426.3595177714365}\n",
      "Iteration 9, Losses: {'ner': 164.19233191771946}\n",
      "Iteration 10, Losses: {'ner': 123.4704915248998}\n",
      "Iteration 11, Losses: {'ner': 115.38630447694601}\n",
      "Iteration 12, Losses: {'ner': 107.13690338553319}\n",
      "Iteration 13, Losses: {'ner': 82.53998280391474}\n",
      "Iteration 14, Losses: {'ner': 116.1989592844416}\n",
      "Iteration 15, Losses: {'ner': 69.82931312230033}\n",
      "Iteration 16, Losses: {'ner': 75.55982103294356}\n",
      "Iteration 17, Losses: {'ner': 72.26496800415195}\n",
      "Iteration 18, Losses: {'ner': 66.88909319751268}\n",
      "Iteration 19, Losses: {'ner': 37.73250009120546}\n",
      "Iteration 20, Losses: {'ner': 34.267038888440084}\n",
      "Iteration 21, Losses: {'ner': 46.8112755598475}\n",
      "Iteration 22, Losses: {'ner': 27.93595738418015}\n",
      "Iteration 23, Losses: {'ner': 41.459815450489174}\n",
      "Iteration 24, Losses: {'ner': 27.696760526997345}\n",
      "Iteration 25, Losses: {'ner': 33.27652399933133}\n",
      "Iteration 26, Losses: {'ner': 33.57898773198466}\n",
      "Iteration 27, Losses: {'ner': 24.83451043458143}\n",
      "Iteration 28, Losses: {'ner': 30.16238871876384}\n",
      "Iteration 29, Losses: {'ner': 43.20744609779483}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "ner.cfg[\"entity_tagger\"] = \"bio\"\n",
    "\n",
    "try:\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.begin_training()  # Initialize the model\n",
    "        optimizer.learn_rate = 0.01\n",
    "        for i in range(30):  # Number of training iterations (adjust as needed)\n",
    "            if i >= 9: \n",
    "                optimizer.learn_rate = 1e-3\n",
    "            if i >= 18:\n",
    "                optimizer.learn_rate = 1e-4\n",
    "            random.shuffle(ner_train_inputs)\n",
    "            losses = {}\n",
    "            batches = minibatch(ner_train_inputs, size=8)\n",
    "            for batch in batches:\n",
    "                for text, annotations in batch:\n",
    "                    doc = nlp.make_doc(text)\n",
    "                    example = Example.from_dict(doc, annotations)\n",
    "                    nlp.update([example], drop=0.1, sgd=optimizer, losses=losses)\n",
    "            print(f\"Iteration {i}, Losses: {losses}\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Losses: {'ner': 19.677851563674444}\n",
      "Iteration 1, Losses: {'ner': 26.02974700061989}\n",
      "Iteration 2, Losses: {'ner': 17.964574074179716}\n",
      "Iteration 3, Losses: {'ner': 19.660696260087306}\n",
      "Iteration 4, Losses: {'ner': 17.445558394918905}\n",
      "Iteration 5, Losses: {'ner': 27.796410740120077}\n",
      "Iteration 6, Losses: {'ner': 23.36597270980115}\n",
      "Iteration 7, Losses: {'ner': 21.427785733789484}\n",
      "Iteration 8, Losses: {'ner': 28.36631520661781}\n",
      "Iteration 9, Losses: {'ner': 24.104098416128178}\n",
      "Iteration 10, Losses: {'ner': 26.763916227431476}\n",
      "Iteration 11, Losses: {'ner': 25.79023606844047}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m                     doc = nlp.make_doc(text)\n\u001b[32m     16\u001b[39m                     example = Example.from_dict(doc, annotations)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m                     \u001b[43mnlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msgd\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Losses: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sides/nergnt/venvv/lib/python3.12/site-packages/spacy/language.py:1193\u001b[39m, in \u001b[36mLanguage.update\u001b[39m\u001b[34m(self, examples, _, drop, sgd, losses, component_cfg, exclude, annotates)\u001b[39m\n\u001b[32m   1190\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, proc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pipeline:\n\u001b[32m   1191\u001b[39m     \u001b[38;5;66;03m# ignore statements are used here because mypy ignores hasattr\u001b[39;00m\n\u001b[32m   1192\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[33m\"\u001b[39m\u001b[33mupdate\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1193\u001b[39m         \u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msgd\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcomponent_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1194\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sgd \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1195\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1196\u001b[39m             name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude\n\u001b[32m   1197\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(proc, ty.TrainableComponent)\n\u001b[32m   1198\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m proc.is_trainable\n\u001b[32m   1199\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m proc.model \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1200\u001b[39m         ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sides/nergnt/venvv/lib/python3.12/site-packages/spacy/pipeline/transition_parser.pyx:439\u001b[39m, in \u001b[36mspacy.pipeline.transition_parser.Parser.update\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sides/nergnt/venvv/lib/python3.12/site-packages/spacy/ml/parser_model.pyx:334\u001b[39m, in \u001b[36mspacy.ml.parser_model.ParserStepModel.finish_steps\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sides/nergnt/venvv/lib/python3.12/site-packages/thinc/layers/chain.py:60\u001b[39m, in \u001b[36mforward.<locals>.backprop\u001b[39m\u001b[34m(dY)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackprop\u001b[39m(dY: OutT) -> InT:\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(callbacks):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m         dX = \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m         dY = dX\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dX\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sides/nergnt/venvv/lib/python3.12/site-packages/thinc/layers/chain.py:60\u001b[39m, in \u001b[36mforward.<locals>.backprop\u001b[39m\u001b[34m(dY)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackprop\u001b[39m(dY: OutT) -> InT:\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(callbacks):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m         dX = \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m         dY = dX\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dX\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sides/nergnt/venvv/lib/python3.12/site-packages/thinc/layers/with_array.py:81\u001b[39m, in \u001b[36m_list_forward.<locals>.backprop\u001b[39m\u001b[34m(dYs)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackprop\u001b[39m(dYs: ListXd) -> ListXd:\n\u001b[32m     80\u001b[39m     dYf = layer.ops.flatten(dYs, pad=pad)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     dXf = \u001b[43mget_dXf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdYf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer.ops.unflatten(dXf, lengths, pad=pad)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sides/nergnt/venvv/lib/python3.12/site-packages/thinc/layers/chain.py:60\u001b[39m, in \u001b[36mforward.<locals>.backprop\u001b[39m\u001b[34m(dY)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackprop\u001b[39m(dY: OutT) -> InT:\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(callbacks):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m         dX = \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m         dY = dX\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dX\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sides/nergnt/venvv/lib/python3.12/site-packages/thinc/layers/residual.py:30\u001b[39m, in \u001b[36mforward.<locals>.backprop\u001b[39m\u001b[34m(d_output)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackprop\u001b[39m(d_output: InT) -> InT:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     dX = \u001b[43mbackprop_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(d_output, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m     32\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [d_output[i] + dX[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(d_output))]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sides/nergnt/venvv/lib/python3.12/site-packages/thinc/layers/chain.py:60\u001b[39m, in \u001b[36mforward.<locals>.backprop\u001b[39m\u001b[34m(dY)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackprop\u001b[39m(dY: OutT) -> InT:\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(callbacks):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m         dX = \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m         dY = dX\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dX\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sides/nergnt/venvv/lib/python3.12/site-packages/thinc/layers/chain.py:60\u001b[39m, in \u001b[36mforward.<locals>.backprop\u001b[39m\u001b[34m(dY)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackprop\u001b[39m(dY: OutT) -> InT:\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(callbacks):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m         dX = \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m         dY = dX\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dX\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sides/nergnt/venvv/lib/python3.12/site-packages/thinc/layers/chain.py:60\u001b[39m, in \u001b[36mforward.<locals>.backprop\u001b[39m\u001b[34m(dY)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackprop\u001b[39m(dY: OutT) -> InT:\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(callbacks):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m         dX = \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m         dY = dX\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dX\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sides/nergnt/venvv/lib/python3.12/site-packages/thinc/layers/layernorm.py:30\u001b[39m, in \u001b[36mforward.<locals>.backprop\u001b[39m\u001b[34m(dY)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackprop\u001b[39m(dY: InT) -> InT:\n\u001b[32m     29\u001b[39m     dY = backprop_rescale(dY)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     dist, sum_dy, sum_dy_dist = \u001b[43m_get_d_moments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     d_xhat = N * dY - sum_dy - dist * var ** (-\u001b[32m1.0\u001b[39m) * sum_dy_dist\n\u001b[32m     32\u001b[39m     d_xhat *= var ** (-\u001b[32m1.0\u001b[39m / \u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sides/nergnt/venvv/lib/python3.12/site-packages/thinc/layers/layernorm.py:85\u001b[39m, in \u001b[36m_get_d_moments\u001b[39m\u001b[34m(ops, dy, X, mu)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_d_moments\u001b[39m(\n\u001b[32m     80\u001b[39m     ops: Ops, dy: Floats2d, X: Floats2d, mu: Floats2d\n\u001b[32m     81\u001b[39m ) -> Tuple[Floats2d, Floats2d, Floats2d]:\n\u001b[32m     82\u001b[39m     dist = X - mu\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m     84\u001b[39m         dist,\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m         \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m,\n\u001b[32m     86\u001b[39m         ops.xp.sum(dy * dist, axis=\u001b[32m1\u001b[39m, keepdims=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m     87\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sides/nergnt/venvv/lib/python3.12/site-packages/numpy/core/fromnumeric.py:2172\u001b[39m, in \u001b[36m_sum_dispatcher\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   2102\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2103\u001b[39m \u001b[33;03m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[32m   2104\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2167\u001b[39m \n\u001b[32m   2168\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   2169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[33m'\u001b[39m\u001b[33mclip\u001b[39m\u001b[33m'\u001b[39m, a_min, a_max, out=out, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2172\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum_dispatcher\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2173\u001b[39m                     initial=\u001b[38;5;28;01mNone\u001b[39;00m, where=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[32m   2177\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[32m   2178\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msum\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=np._NoValue,\n\u001b[32m   2179\u001b[39m         initial=np._NoValue, where=np._NoValue):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.resume_training()  # Initialize the model\n",
    "        optimizer.learn_rate = 1e-8\n",
    "        for i in range(30):  # Number of training iterations (adjust as needed)\n",
    "            if i >= 9: \n",
    "                optimizer.learn_rate = 1e-9\n",
    "            if i >= 18:\n",
    "                optimizer.learn_rate = 1e-10\n",
    "            random.shuffle(ner_train_inputs)\n",
    "            losses = {}\n",
    "            batches = minibatch(ner_train_inputs, size=8)\n",
    "            for batch in batches:\n",
    "                for text, annotations in batch:\n",
    "                    doc = nlp.make_doc(text)\n",
    "                    example = Example.from_dict(doc, annotations)\n",
    "                    nlp.update([example], drop=0.1, sgd=optimizer, losses=losses)\n",
    "            print(f\"Iteration {i}, Losses: {losses}\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk('./odycy_with_ner/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer',\n",
       " 'tagger',\n",
       " 'morphologizer',\n",
       " 'parser',\n",
       " 'trainable_lemmatizer',\n",
       " 'frequency_lemmatizer']"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('grc_odycy_joint_trf')\n",
    "# ner = nlp.create_pipe('ner')\n",
    "# ner.from_disk('./ner_model')\n",
    "\n",
    "# ner_pipe = nlp.get_pipe('ner')\n",
    "# ner_pipe.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Παῦλος δὲ καὶ \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Βαρναβᾶς\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " διέτριβον ἐν \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ἀντιοχείᾳ\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " διδάσκοντες καὶ εὐαγγελιζόμενοι μετὰ καὶ ἑτέρων πολλῶν τὸν λόγον τοῦ \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    κυρίου\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TITLE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verse id: 51535\n",
      "Correct annotations:\n",
      "{'entities': [(0, 6, 'PERSON'), (14, 22, 'PERSON'), (36, 45, 'GPE'), (115, 121, 'TITLE')]}\n"
     ]
    }
   ],
   "source": [
    "idx = 298\n",
    "text, annotations = ner_test_inputs[idx]\n",
    "verse_id = test_verse_ids[idx]\n",
    "doc = nlp(text)\n",
    "\n",
    "spacy.displacy.render(doc, style='ent')\n",
    "print('Verse id:', verse_id)\n",
    "print('Correct annotations:')\n",
    "print(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translation (ESV): But Paul and Barnabas remained in Antioch, teaching and preaching the word of the Lord, with many others also. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.scorer import Scorer\n",
    "scorer = Scorer()\n",
    "examples = []\n",
    "for text, annot in ner_test_inputs:\n",
    "    doc = nlp(text)\n",
    "    example = Example.from_dict(nlp.make_doc(text), annot)\n",
    "    examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = scorer.score(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 1.0,\n",
       " 'token_p': 1.0,\n",
       " 'token_r': 1.0,\n",
       " 'token_f': 1.0,\n",
       " 'sents_p': None,\n",
       " 'sents_r': None,\n",
       " 'sents_f': None,\n",
       " 'tag_acc': None,\n",
       " 'pos_acc': None,\n",
       " 'morph_acc': None,\n",
       " 'morph_micro_p': None,\n",
       " 'morph_micro_r': None,\n",
       " 'morph_micro_f': None,\n",
       " 'morph_per_feat': None,\n",
       " 'dep_uas': None,\n",
       " 'dep_las': None,\n",
       " 'dep_las_per_type': None,\n",
       " 'ents_p': 0.0,\n",
       " 'ents_r': 0.0,\n",
       " 'ents_f': 0.0,\n",
       " 'ents_per_type': {'GROUP': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'ORG': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'TITLE': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'DIVINE': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'GPE': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'PERSON': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'LOC': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'LANG': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'EVENT': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'DEITY': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'BOOK': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'TIME': {'p': 0.0, 'r': 0.0, 'f': 0.0}},\n",
       " 'cats_score': 0.0,\n",
       " 'cats_score_desc': 'macro F',\n",
       " 'cats_micro_p': 0.0,\n",
       " 'cats_micro_r': 0.0,\n",
       " 'cats_micro_f': 0.0,\n",
       " 'cats_macro_p': 0.0,\n",
       " 'cats_macro_r': 0.0,\n",
       " 'cats_macro_f': 0.0,\n",
       " 'cats_macro_auc': 0.0,\n",
       " 'cats_f_per_type': {},\n",
       " 'cats_auc_per_type': {}}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
